<?xml version="1.0" encoding="UTF-8"?>
<!--
    scan:当此属性设置为true时，配置文件如果发生改变，将会被重新加载，默认值为true。
    scanPeriod:设置监测配置文件是否有修改的时间间隔，如果没有给出时间单位，默认单位是毫秒。当scan为true时，此属性生效。默认的时间间隔为1分钟。
    debug:当此属性设置为true时，将打印出logback内部日志信息，实时查看logback运行状态。默认值为false。
-->
<configuration>

    <!--
        每个logger都关联到logger上下文，默认上下文名称为“default”。但可以使用设置成其他名字，用于区分不同应用程序的记录。
        一旦设置，不能修改,可以通过%contextName来打印日志上下文名称。
    -->
    <contextName>order-api</contextName>

    <!-- 属性文件
     用来定义变量值的标签， 有两个属性，name和value；其中name的值是变量的名称，value的值时变量定义的值。
     通过定义的值会被插入到logger上下文中。定义变量后，可以使“${}”来使用变量。
     -->
    <property name="log.path" value="/data/webdev/logs/order-api"/>


    <!-- 默认的控制台日志输出，一般生产环境都是后台启动，这个没太大作用
     appender用来格式化日志输出节点，有俩个属性name和class，class用来指定哪种输出策略，常用就是控制台输出策略和文件输出策略。
     -->
    <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
        <!--<encoder>表示对日志进行编码：
                %d{HH: mm:ss.SSS}——日志输出时间
                %thread——输出日志的进程名字，这在Web应用以及异步任务处理中很有用
                %-5level——日志级别，并且使用5个字符靠左对齐
                %logger{36}——日志输出者的名字
                %msg——日志消息
                %n——平台的换行符
        -->
        <encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder">
            <!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度%msg：日志消息，%n是换行符-->
            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS}  [%thread] %-5level %logger{50} - %msg%n</pattern>
            <charset>utf-8</charset>
        </encoder>
    </appender>


    <!-- 配置文件轮转
     另一种常见的日志输出到文件，随着应用的运行时间越来越长，日志也会增长的越来越多，
     将他们输出到同一个文件并非一个好办法。RollingFileAppender用于切分文件日志
     -->
    <appender name="logfile" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <!--其中重要的是rollingPolicy的定义，上例中
        <fileNamePattern>${log.path}/logback.%d{yyyy-MM-dd}.log</fileNamePattern>定义了日志的切分方式——把每一天的日志归档到一个文件中，
        <maxHistory>30</maxHistory>表示只保留最近30天的日志，以防止日志填满整个磁盘空间。同理，可以使用%d{yyyy-MM-dd_HH-mm}来定义精确到分的日志切分方式。
        <totalSizeCap>1GB</totalSizeCap>用来指定日志文件的上限大小，例如设置为1GB的话，那么到了这个值，就会删除旧的日志。-->
        <File>${log.path}/server.log</File>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log.path}/%d{yyyy-MM-dd}/order-api.%d{yyyy-MM-dd}.log.gz</fileNamePattern>
            <maxHistory>30</maxHistory>
            <totalSizeCap>1GB</totalSizeCap>
        </rollingPolicy>
        <encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder">
            <!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度%msg：日志消息，%n是换行符-->
            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg%n</pattern>
            <charset>utf-8</charset>
        </encoder>
    </appender>

    <appender name ="ASYNC" class= "ch.qos.logback.classic.AsyncAppender">
        <!-- 不丢失日志.默认的,如果队列的80%已满,则会丢弃TRACT、DEBUG、INFO级别的日志 -->
        <discardingThreshold>0</discardingThreshold>
        <!-- 更改默认的队列的深度,该值会影响性能.默认值为256 -->
        <queueSize>256</queueSize>
        <!-- 添加附加的appender,最多只能添加一个 -->
        <appender-ref ref ="logfile"/>
    </appender>

    <!--<appender name="KAFKA" class="com.fang.order.common.config.kafka.KafkaAppender">-->
    <!--<encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder">-->
    <!--<pattern>%d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n</pattern>-->
    <!--</encoder>-->
    <!--<topic>dingdan</topic>-->
    <!--<keyingStrategy class="com.fang.order.common.config.kafka.keying.NoKeyKeyingStrategy"></keyingStrategy>-->

    <!--<deliveryStrategy class="com.fang.order.common.config.kafka.delivery.AsynchronousDeliveryStrategy"></deliveryStrategy>-->

    <!--<producerConfig>bootstrap.servers=10.32.130.42:9092,10.32.130.43:9092</producerConfig>-->
    <!--<producerConfig>acks=0</producerConfig>-->
    <!--<producerConfig>linger.ms=1000</producerConfig>-->
    <!--<producerConfig>max.block.ms=0</producerConfig>-->
    <!--<producerConfig>client.id=${HOSTNAME}-${CONTEXT_NAME}-logback-relaxed</producerConfig>-->
    <!--</appender>-->

    <!--&lt;!&ndash;配置自定义日志输出类&ndash;&gt;-->
    <!--<appender name="KAFKA" class="com.fang.order.common.config.kafka.KafkaAppender">-->
    <!--<topic>dingdan</topic>-->
    <!--<brokerList>10.32.130.42:9092,10.32.130.43:9092</brokerList>-->
    <!--<formatter class="com.fang.order.common.config.kafka.JsonFormatter">-->
    <!--<expectJson>false</expectJson>-->
    <!--</formatter>-->
    <!--</appender>-->


    <!--
        <logger>用来设置某一个包或者具体的某一个类的日志打印级别、以及指定<appender>。<logger>仅有一个name属性，
                一个可选的level和一个可选的addtivity属性。
        name:用来指定受此logger约束的某一个包或者具体的某一个类。
        level:用来设置打印级别，大小写无关：TRACE, DEBUG, INFO, WARN, ERROR, ALL 和 OFF，还有一个特俗值INHERITED或者同义词NULL，
                代表强制执行上级的级别。如果未设置此属性，那么当前logger将会继承上级的级别。
        addtivity:是否向上级logger传递打印信息。默认是true。
    -->
    <logger name="com.fang.order" additivity="false" level="${log.level:-INFO}">
        <appender-ref ref="STDOUT" />
        <appender-ref ref="ASYNC"/>
        <!--<appender-ref ref="KAFKA"/>-->
    </logger>
    <!--<logger name="com.fang.order.dao" additivity="false" level="DEBUG">-->
    <!--<appender-ref ref="STDOUT" />-->
    <!--&lt;!&ndash;<appender-ref ref="ASYNC"/>&ndash;&gt;-->
    <!--&lt;!&ndash;<appender-ref ref="KAFKA"/>&ndash;&gt;-->
    <!--</logger>-->
    <!--
        root节点是必选节点，用来指定最基础的日志输出级别，只有一个level属性。
        level:用来设置打印级别，大小写无关：TRACE, DEBUG, INFO, WARN, ERROR, ALL 和 OFF，不能设置为INHERITED或者同义词NULL。
        默认是DEBUG。
        可以包含零个或多个元素，标识这个appender将会添加到这个logger。
    -->
    <root level="${log.level:-INFO}">
        <appender-ref ref="logfile"/>
        <appender-ref ref="STDOUT" />
        <!--<appender-ref ref="KAFKA"/>-->
    </root>
</configuration>